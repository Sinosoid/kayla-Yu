1
00:00:00,000 --> 00:00:03,500
Multiparadigm Data Science
with the Wolfram Language,

2
00:00:03,500 --> 00:00:06,600
Section 1: The Project Workflow.

3
00:00:06,600 --> 00:00:09,000
We are working on building
a project workflow

4
00:00:09,000 --> 00:00:11,400
for our example
data science project,

5
00:00:11,400 --> 00:00:12,800
and we have completed the first

6
00:00:12,800 --> 00:00:15,600
and very important stage
of the workflow:

7
00:00:15,600 --> 00:00:17,700
setting up questions
that can be answered

8
00:00:17,700 --> 00:00:19,100
with the help of the data.

9
00:00:19,100 --> 00:00:20,100
In the next stage,

10
00:00:20,100 --> 00:00:22,800
we will actually get
our hands on the data.

11
00:00:22,800 --> 00:00:25,300
Data wrangling involves
importing raw data

12
00:00:25,300 --> 00:00:27,800
and converting it
into a suitable shape

13
00:00:27,800 --> 00:00:30,500
for the rest of the analysis.

14
00:00:30,500 --> 00:00:33,000
To get the tweets
made by Wolfram Research,

15
00:00:33,000 --> 00:00:35,700
we will connect to the Twitter API

16
00:00:35,700 --> 00:00:39,600
and use that connection
to download the tweets.

17
00:00:39,600 --> 00:00:42,400
We have set the username
to be WolframResearch

18
00:00:42,400 --> 00:00:45,500
because this is the Twitter
account we are interested in.

19
00:00:47,500 --> 00:00:51,300
Let's take a peek and see
how the data is structured.

20
00:00:51,300 --> 00:00:54,000
Here are five random samples.

21
00:00:54,000 --> 00:00:56,000
Looking at the fifth row closely,

22
00:00:56,000 --> 00:00:59,900
we see the tweets
each have a unique ID;

23
00:00:59,900 --> 00:01:03,500
the text of the tweet;
something called thumbnailsURL;

24
00:01:03,500 --> 00:01:05,700
date and time
when the tweet was made;

25
00:01:05,700 --> 00:01:07,800
hashtags used in the tweet;

26
00:01:07,800 --> 00:01:10,400
location of the tweet,
which can be empty;

27
00:01:10,400 --> 00:01:12,800
and the user who made it,
which is redundant

28
00:01:12,800 --> 00:01:14,100
because we know
we downloaded tweets

29
00:01:14,100 --> 00:01:16,150
only made by WolframResearch.

30
00:01:16,150 --> 00:01:19,300
Same case with the name
and the ProfileImageThumbnail.

31
00:01:19,300 --> 00:01:21,000
But these two items
should be useful,

32
00:01:21,000 --> 00:01:23,500
the retweet count
and the favorite count,

33
00:01:23,500 --> 00:01:25,500
which can be used as indicators

34
00:01:25,500 --> 00:01:28,500
of engagement
generated by the tweets.

35
00:01:28,500 --> 00:01:32,600
Lastly, there is the URL to find
the tweet on the Twitter site.

36
00:01:32,600 --> 00:01:35,000
This is the underlying structure
of each row:

37
00:01:35,000 --> 00:01:38,300
it's an association
of key-value pairs—

38
00:01:38,300 --> 00:01:40,100
key being the feature name

39
00:01:40,100 --> 00:01:43,700
and value being the feature value
for this particular sample.

40
00:01:44,800 --> 00:01:47,900
One piece of information
not included explicitly

41
00:01:47,900 --> 00:01:52,800
in this structured data right now,
but which might be useful later on

42
00:01:52,800 --> 00:01:56,500
is the list of user mentions
found in the tweet.

43
00:01:56,500 --> 00:01:58,100
But we can parse that

44
00:01:58,100 --> 00:02:01,000
out of the text of the tweets
to get that information.

45
00:02:02,000 --> 00:02:05,200
For example, say this is
a piece of text from a tweet

46
00:02:05,200 --> 00:02:07,600
and we see Wolfram U
is mentioned here.

47
00:02:07,600 --> 00:02:11,100
We can use string pattern matching
to check for the @ symbol

48
00:02:11,100 --> 00:02:13,600
followed by a digit
or letter or underscore—

49
00:02:13,600 --> 00:02:16,300
basically, the rules
that limit a Twitter handle—

50
00:02:16,300 --> 00:02:17,400
and if we find a match,

51
00:02:17,400 --> 00:02:20,300
we pull it out
to be a user mention.

52
00:02:20,300 --> 00:02:23,000
This function here
uses this pattern matching

53
00:02:23,000 --> 00:02:24,650
to get the user mentions

54
00:02:24,650 --> 00:02:28,000
and adds them
back into the data structure.

55
00:02:28,000 --> 00:02:29,300
Let's use it on an example.

56
00:02:29,300 --> 00:02:33,200
Here is the original association,
and here is the same association,

57
00:02:33,200 --> 00:02:36,100
but now with the user mentions
added in.

58
00:02:36,100 --> 00:02:39,950
We can augment our entire dataset
with the additional information

59
00:02:39,950 --> 00:02:43,300
about user mentions
in an easily accessible form.

60
00:02:44,800 --> 00:02:47,000
Looks like the data
is in pretty good shape.

61
00:02:48,000 --> 00:02:50,500
Sometimes an additional
data cleaning stage

62
00:02:50,500 --> 00:02:52,000
may be needed in the workflow

63
00:02:52,000 --> 00:02:54,600
to deal with things
like missing data,

64
00:02:54,600 --> 00:02:56,000
errors in data, et cetera,

65
00:02:56,000 --> 00:03:00,400
or we could just sneak it into
the data wrangling stage itself.

66
00:03:00,400 --> 00:03:03,000
Here is an example of a tweet,
and we can see it has

67
00:03:03,000 --> 00:03:07,000
some characters that are not
really useful for text analysis.

68
00:03:07,000 --> 00:03:12,600
We can easily remove the "& gt",
"& amp" and the URLs

69
00:03:12,600 --> 00:03:15,300
with some more
string pattern matching.

70
00:03:15,300 --> 00:03:21,300
And then we can delete stopwords
like "is," "the," "of," et cetera.

71
00:03:21,300 --> 00:03:23,900
We wrap up these two steps
into a single function

72
00:03:23,900 --> 00:03:26,800
that will clean up
the text from a tweet.

73
00:03:26,800 --> 00:03:30,100
So it seems like we have
our data in shape

74
00:03:30,100 --> 00:03:33,600
and we have some code to
clean up the text if we need to.

75
00:03:33,600 --> 00:03:35,800
We are ready to move on
to the next stage,

76
00:03:35,800 --> 00:03:37,700
which is really
the most interesting part,

77
00:03:37,700 --> 00:03:40,000
because it is exploration.

